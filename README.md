# hadoop-mapreduce

udacity course projects and memos

### Hadoop

* a way to store data, known as the Hadoop distributed file system, or HDFS

* a way to process data with MapReduce

**key concept**: We split the data up and store it across the  collection of machines known as a **_cluster_**. We process it where it's actually installed.
You can add more machines to the cluster as the amount of data you're storing grows.


### Hadoop Ecosystem
* Some software is intended to make it easier to load the data into the Hadoop cluster. 
* Others make the Hadoop easier to use.

To some one who aren't programmer, but can write SQL queries.
* Hive: The hive interpreter turns the SQL into map produce code, which the runs on the cluster.
* Pig: allow you to analyze your code in a simple scripting language, and the codes turn into map reduce and run on a cluster.
    
Hive and Pig are suitable for running long batch processing.

* Impala: a way to query your data with SQL, but which directly accesses the data in HDFS rather than needing map reduce. Suitable for low latency queries.
* HBase: a real time database, built on top of HDFS.
Sqoop: takes data from a traditional relational database, and puts it in HDFS, as the limited files. So it can be processed along with other data on the cluster.
* Flume: injests data as it's generated by external systems, and again puts it into the cluster.

* Hue: graphical front end to the questor.
* Oozie: workflow management tool.
* Mahout is a machine learning library.


Cloudera has put together a distribution of Hadoop.





